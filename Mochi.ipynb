{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Mochi.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DG1dEeHtPcnR"},"source":["**Mochi Toxic Comments Classification Codes**</br>\n","last update: Dec/08 7:21 AM</br>\n","</br>\n","This notebook contains code for group Mochi's project</br>\n","Following codes are included</br>\n","0. set up hyperparameters</br>\n","1. Prepossessing Data</br>\n"," 1. extract data from dataset</br>\n"," 2. divide into training, testing, validation datasets</br>\n"," 3. do weighting (if weighting = True)</br>\n"," 4. save dataLoader into file</br>\n","2. Define Model and Train Model</br>\n"," 1. load dataLoader from file</br>\n"," 2. define model</br>\n"," 3. train model</br>\n","3. Evaluate Model</br>\n"," 1. evaluate model using testing dataset</br>\n"," 2. save hyparameters and evaluations into file</br>\n","\n","note:</br>\n","1. multi labeling input -> [0,0,1,1,0]\n","2. multi labeling output ->[0.001, 0.01, 0.6, 0.7, 0.0001]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87b4NbYJdRlz","outputId":"a8a8329f-3ec6-4af1-e39c-f4db74501f39"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import csv\n","import torch\n","from torch import nn\n","import numpy as np\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","import torch.nn.functional as F\n","import tensorflow as tf\n","from torch.utils import data\n","from tqdm import tqdm\n","import random\n","import pickle\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"HghdOHzdtjgw"},"source":["#https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n","#Download data from the above url, and upload it in the google drive \"Mochi\" folder\n","#test = 182375\n","#train = 1684759\n","#used only approved data[7], and threshold is set to 0.5 with category \"toxicity\"[13]\n","BASE_DIR = '/content/drive/My Drive/Mochi'\n","src_file = os.path.join(BASE_DIR,\"all_data.csv\")\n","temp = False # True for test(small size) False for real(big size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7GeYIMVwSvt"},"source":["#https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n","#Download data from the above url, and upload it in the google drive \"Mochi\" folder\n","#test = 182375\n","#train = 1684759\n","#used only approved data[7], and threshold is set to 0.5 with category \"toxicity\"[13]\n","#returns train/test/val_toxic/type/target each array represents classes(toxic/non-toxic, types of toxic, targets of toxic)\n","#more detail about the classes are below\n","def csv_processing(src_file, temp):\n","  tox = 0\n","  norm = 0\n","  test_set = True\n","\n","  f = open(src_file, 'r', encoding='utf-8')\n","  src = csv.reader(f)\n","  train = []\n","  train_toxic = []\n","  train_type = []\n","  train_target = []\n","  test = []\n","  test_toxic = []\n","  test_type = []\n","  test_target = []\n","  val = []\n","  val_toxic = []\n","  val_type = []\n","  val_target = []\n","  types = []\n","  targets = []\n","  if temp == True:\n","    rate = 0.1#(10% of train data)\n","  else:\n","    rate = 0.01#(1% of train data)\n","  for count, line in enumerate(src):\n","    if count == 0:\n","      types = types + line[14:20]\n","      targets = targets + line[20:44]\n","      continue\n","    if float(line[13]) == 0.0 and temp == True: ##temp for reducing the size of train set\n","      break\n","    if line[7] == 'approved':\n","      if line[2] == 'train':\n","        if random.random()>rate: #0.1 for temp, 0.01 for real\n","          train.append(line[1])\n","          binary, toxic_type, target = classification(line)\n","          train_toxic.append(binary)\n","          train_type.append(toxic_type)\n","          train_target.append(target)\n","        else:\n","          val.append(line[1])\n","          binary, toxic_type, target = classification(line)\n","          val_toxic.append(binary)\n","          val_type.append(toxic_type)\n","          val_target.append(target)\n","      if line[2] == 'test' and test_set == True:\n","        if float(line[13]) >= 0.5:\n","          tox = tox +1\n","        else:\n","          norm = norm +1\n","        if float(line[13]) == 0.0 and tox-norm == 0:\n","          test_set = False\n","        test.append(line[1])\n","        binary, toxic_type, target = classification(line)\n","        test_toxic.append(binary)\n","        test_type.append(toxic_type)\n","        test_target.append(target)\n","    \n","  train_toxic = torch.tensor(train_toxic)\n","  test_toxic = torch.tensor(test_toxic)\n","  val_toxic = torch.tensor(val_toxic)\n","  train_type = torch.tensor(train_type)\n","  test_type = torch.tensor(test_type)\n","  val_type = torch.tensor(val_type)\n","  train_target = torch.tensor(train_target)\n","  test_target = torch.tensor(test_target)\n","  val_target = torch.tensor(val_target)\n","  f.close()\n","\n","  return train, train_toxic, train_type, train_target, test, test_toxic, test_type, test_target, val, val_toxic, val_type, val_target, types, targets\n","\n","def classification(line):\n","  toxic_type = [0,0,0,0,0,0]\n","  target = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","  if float(line[13]) >= 0.5:\n","    binary = 1\n","    type_value = max(line[14:20])\n","    for i in range(14,20):\n","      if float(line[i]) >= 0.5:\n","        toxic_type[i-14] = 1\n","      else:\n","        toxic_type[i-14] = 0\n","    if line[20] != '':\n","      for i in range(20,44):\n","        if float(line[i]) >= 0.5:\n","          target[i-20] = 1\n","        else:\n","          target[i-20] = 0\n","  else:\n","    binary = 0\n","\n","  return binary, toxic_type, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsy1MBuDP9JJ"},"source":["train, train_toxic, train_type, train_target, test, test_toxic, test_type, test_target, val, val_toxic, val_type, val_target, types, targets = csv_processing(src_file, temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEKytuzbkY4J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3ea6f29-57d7-4413-9c7c-fc6997ce2da3"},"source":["#more details about the classes to be classified\n","print(len(train))\n","print(len(test))\n","print(len(val))\n","print(types) # each index represents the class of train/test/val_type\n","print(targets)# each index represents the class of train/test/val_targets\n","print(len(types))\n","print(len(targets))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1667834\n","1098\n","16925\n","['severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']\n","['male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n","6\n","24\n"]}]},{"cell_type":"code","metadata":{"id":"H8_W7MlA7sGU"},"source":["special_chars = ['`','~','!','@','#','^','(',')','[','{','}',']',';',':','*','$','%','&','-','_','=','+','\\'','\"','\\\\','\\n','\\t','|',',','<','.','>','/','?']\n","def sp_char_processing(data_set, special_chars):\n","  for i in range(len(data_set)):\n","    text = data_set[i]\n","    for char in special_chars:\n","      text = text.replace(char,'')\n","    data_set[i] = \"<sos> \" + text + \" <eos>\"\n","\n","  return data_set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJ-JeAWruJco"},"source":["#https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n","#constructing our own vocabulary\n","train = sp_char_processing(train, special_chars)\n","test = sp_char_processing(test, special_chars)\n","val = sp_char_processing(val, special_chars)\n","tokenizer = get_tokenizer('basic_english')\n","\n","def yield_tokens(data_iter):\n","    for text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train), specials=[\"<unk>\",'<pad>','<sos>','<eos>']) #0, 1, 2, 3 ,min_freq = 1\n","vocab.set_default_index(vocab[\"<unk>\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4McgAfXILM6W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7d4c1a4-04a6-4c3b-b5c2-eafdd1101804"},"source":["# visualizing the dataset\n","print(train[5000])\n","print(tokenizer(train[5000]))\n","print(vocab(tokenizer(train[5000])))\n","print(test[1])\n","print(vocab(tokenizer(test[1])))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<sos> It is the same idiots who reduced the Old Age Security pension qualifying age from 67 to 65 yearsCanadians elected a self destructive Government believing in heart outward economy and self balancing budget headed by a juvenile drama king  They are getting what they deserved <eos>\n","['<sos>', 'it', 'is', 'the', 'same', 'idiots', 'who', 'reduced', 'the', 'old', 'age', 'security', 'pension', 'qualifying', 'age', 'from', '67', 'to', '65', 'yearscanadians', 'elected', 'a', 'self', 'destructive', 'government', 'believing', 'in', 'heart', 'outward', 'economy', 'and', 'self', 'balancing', 'budget', 'headed', 'by', 'a', 'juvenile', 'drama', 'king', 'they', 'are', 'getting', 'what', 'they', 'deserved', '<eos>']\n","[2, 14, 9, 4, 118, 2558, 38, 1880, 4, 251, 681, 609, 1537, 12262, 681, 36, 6238, 5, 3208, 287344, 497, 8, 912, 4089, 103, 3112, 10, 1109, 15187, 427, 6, 912, 7919, 504, 3380, 35, 8, 5355, 3494, 1673, 23, 16, 265, 34, 23, 4496, 3]\n","<sos> NO   There are no alternative facts Go check for yourself It is people like you who have no idea what you are talking about that has gotten this State and Country into the mess it is in People who think the Goverment be it State or Federal can spend the peoples money better than they can is stupid and nonsensical Politicians use taxes as Personal slush accounts to continue their carrers buying votes from the lame and the lazy <eos>\n","[2, 42, 51, 16, 42, 1322, 389, 111, 716, 12, 562, 14, 9, 46, 54, 13, 38, 19, 42, 347, 34, 13, 16, 441, 45, 11, 44, 2075, 22, 109, 6, 144, 105, 4, 1099, 14, 9, 10, 46, 38, 78, 4, 14461, 18, 14, 109, 26, 283, 55, 583, 4, 981, 108, 138, 69, 23, 55, 9, 649, 6, 7036, 534, 155, 238, 24, 458, 12725, 2280, 5, 412, 33, 409089, 1188, 738, 36, 4, 3774, 6, 4, 2639, 3]\n"]}]},{"cell_type":"code","metadata":{"id":"dOdP34DiKaFt"},"source":["##############################\n","######hyperparameters#########\n","##############################\n","classification = 'binary' #one of 'binary', 'type', 'target'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","embed_dim = 256\n","max_len = 100 #train avg 61, test avg 55\n","batch_size = 512\n","hidden_size = 500\n","learning_rate = 0.001\n","num_layer = 1\n","bidirectional = True\n","num_epochs = 4\n","if classification == 'binary':\n","  num_classes = 1 # 1 for binary, 6 for types, 24 for targets\n","elif classification == 'type':\n","  num_classes = 6\n","elif classification == 'target':\n","  num_classes = 24\n","use_GLoVe = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xq6kNseoXSuW"},"source":["#tokenizing + indexing + maxlen filtering\n","# and padding\n","#\n","# After,  tokened is sentence in word tokens (each sentence has 100 word tokens)\n","#         bin_set is sentence labels (toxic or not) ([1,0] for toxic, [0,1] for not toxic) (binary type targets)\n","#         data_set is a list of sentences (in words)\n","def tokenizing(data_set, class_set, classification):\n","  tokened = torch.zeros(len(data_set),max_len, dtype=int)\n","  class_sett = class_set.clone()\n","  original = []\n","  count = 0\n","  for i in range(len(data_set)):\n","    if i % int(len(data_set)*0.1) == 0:\n","      print(str(i) + \"/\" + str(len(data_set)))\n","    token = tokenizer(data_set[i])\n","    if len(token) > max_len:\n","      continue\n","    if (classification == \"type\" or classification == \"target\") and sum(class_set[i]) == 0:\n","      continue\n","    token = torch.tensor(vocab(token), dtype=int)\n","    token = F.pad(token, pad=(0, max_len - len(token)), mode='constant', value=vocab(['<pad>'])[0])\n","    tokened[count] = token\n","    class_sett[count] = class_set[i]\n","    original.append(data_set[i])\n","    count = count + 1\n","  print(\"done\")\n","  print(count)\n","  tokened = tokened[:count,:]\n","  class_sett = class_sett[:count]\n","  data_set = original\n","  return tokened, class_sett, data_set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBL_0YVXs5op","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4235360a-4ade-43f1-e8bf-1ec23c46b012"},"source":["# the direct input to the RNN model\n","# constructing the loader of the data sets\n","if classification == \"binary\":\n","  train_set = train_toxic\n","  test_set = test_toxic\n","  val_set = val_toxic\n","elif classification == \"type\":\n","  train_set = train_type\n","  test_set = test_type\n","  val_set = val_type\n","elif classification == \"target\":\n","  train_set = train_target\n","  test_set = test_target\n","  val_set = val_target\n","\n","train_tokened, train_sett, _ = tokenizing(train, train_set, classification)\n","train_dataset = data.TensorDataset(train_tokened, train_sett)\n","train_loader = data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n","\n","test_tokened, test_sett, _ = tokenizing(test, test_set, classification)\n","test_dataset = data.TensorDataset(test_tokened, test_sett)\n","test_loader = data.DataLoader(dataset = test_dataset, batch_size = 1, shuffle = True)\n","\n","val_tokened, val_sett, _ = tokenizing(val, val_set, classification)\n","val_dataset = data.TensorDataset(val_tokened, val_sett)\n","val_loader = data.DataLoader(dataset = val_dataset, batch_size = int(batch_size/2), shuffle = True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/1667688\n","166768/1667688\n","333536/1667688\n","500304/1667688\n","667072/1667688\n","833840/1667688\n","1000608/1667688\n","1167376/1667688\n","1334144/1667688\n","1500912/1667688\n","1667680/1667688\n","done\n","1401455\n","0/1098\n","109/1098\n","218/1098\n","327/1098\n","436/1098\n","545/1098\n","654/1098\n","763/1098\n","872/1098\n","981/1098\n","1090/1098\n","done\n","997\n","0/17071\n","1707/17071\n","3414/17071\n","5121/17071\n","6828/17071\n","8535/17071\n","10242/17071\n","11949/17071\n","13656/17071\n","15363/17071\n","17070/17071\n","done\n","14295\n"]}]},{"cell_type":"code","metadata":{"id":"mrQDQiAGmpET"},"source":["# saving dataLoaders into files, so dont need to tokenlize data everytime.\n","if classification == \"binary\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"BinarydataLoader\")\n","elif classification == \"type\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"TypedataLoader\")\n","elif classification == \"target\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"TargetdataLoader\")\n","\n","if(not os.path.isdir(data_loader_dir)):\n","  os.mkdir(data_loader_dir)\n","\n","torch.save(train_loader, os.path.join(data_loader_dir,\"train_loader.loader\"))\n","torch.save(test_loader, os.path.join(data_loader_dir,\"test_loader.loader\"))\n","torch.save(val_loader, os.path.join(data_loader_dir,\"val_loader.loader\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCUzB7p1Z23K","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"a631b927-fb43-449c-a406-9a98ecc3023c"},"source":["#https://nlp.stanford.edu/projects/glove/\n","# here is GLoVe embedding\n","# implementation follows https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n","# about 80% words in dataset is not in Glove, not using it\n","'''\n","words = []\n","idx = 0\n","word2idx = {}\n","vectors = []\n","\n","with open(os.path.join(BASE_DIR,\"glove.42B.300d.txt\"), 'rb') as f:\n","    for l in f:\n","        line = l.decode().split()\n","        word = line[0]\n","        words.append(word)\n","        word2idx[word] = idx\n","        idx += 1\n","        vect = np.array(line[1:]).astype(np.float)\n","        vectors.append(vect)\n","\n","glove = {}\n","found_words = 0\n","unfound_words = 0\n","vocab_stoi = vocab.get_stoi()\n","for w in vocab_stoi:\n","  if w in word2idx:\n","    found_words += 1\n","    glove[vocab_stoi[w]] = vectors[word2idx[w]]\n","  else:\n","    unfound_words += 1\n","    glove[vocab_stoi[w]] = np.random.normal(scale=0.6, size=100)\n","\n","print(\"found words:\", found_words)\n","print(\"unfound words\", unfound_words)\n","\n","file = open(os.path.join(BASE_DIR,\"glove.42B.300d.embedding_dict\"), \"wb\")\n","pickle.dump(glove, file)\n","file.close()\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nwords = []\\nidx = 0\\nword2idx = {}\\nvectors = []\\n\\nwith open(os.path.join(BASE_DIR,\"glove.42B.300d.txt\"), \\'rb\\') as f:\\n    for l in f:\\n        line = l.decode().split()\\n        word = line[0]\\n        words.append(word)\\n        word2idx[word] = idx\\n        idx += 1\\n        vect = np.array(line[1:]).astype(np.float)\\n        vectors.append(vect)\\n\\nglove = {}\\nfound_words = 0\\nunfound_words = 0\\nvocab_stoi = vocab.get_stoi()\\nfor w in vocab_stoi:\\n  if w in word2idx:\\n    found_words += 1\\n    glove[vocab_stoi[w]] = vectors[word2idx[w]]\\n  else:\\n    unfound_words += 1\\n    glove[vocab_stoi[w]] = np.random.normal(scale=0.6, size=100)\\n\\nprint(\"found words:\", found_words)\\nprint(\"unfound words\", unfound_words)\\n\\nfile = open(os.path.join(BASE_DIR,\"glove.42B.300d.embedding_dict\"), \"wb\")\\npickle.dump(glove, file)\\nfile.close()\\n'"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"PmnW9Ds4neGs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"35b1b72c-9cf0-434c-8385-5edd51103064"},"source":["# load dataLoaders from file\n","if classification == \"binary\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"BinarydataLoader\")\n","elif classification == \"type\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"TypedataLoader\")\n","elif classification == \"target\":\n","  data_loader_dir = os.path.join(BASE_DIR,\"TargetdataLoader\")\n","\n","train_loader = torch.load(os.path.join(data_loader_dir,\"train_loader.loader\"))\n","test_loader = torch.load(os.path.join(data_loader_dir,\"test_loader.loader\"))\n","val_loader = torch.load(os.path.join(data_loader_dir,\"val_loader.loader\"))\n","print(data_loader_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Mochi/TypedataLoader\n"]}]},{"cell_type":"code","metadata":{"id":"Ve2rqmaYzOgF"},"source":["#distribution and weight calculation\n","if classification == 'type':\n","  indexes = [0,0,0,0,0,0]\n","  for (_,labels) in train_loader:\n","    for label in labels:\n","      for i in range(num_classes):\n","        if label[i] != 0:\n","          indexes[i] += 1\n","  print(indexes)\n","elif classification == 'target':\n","  indexes = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","  for (_,labels) in train_loader:\n","    for label in labels:\n","      for i in range(num_classes):\n","        if label[i] != 0:\n","          indexes[i] += 1\n","  print(indexes)\n","\n","#for i in range(num_classes):\n","#  indexes[i] = 1/indexes[i]\n","#weights = indexes # for unbalanced data.(larger classes have less weights)). This is for criterion parameter in training code sectiion\n","#weights[0] = 0.0\n","#class_weights = torch.FloatTensor(weights).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOiRtN7S3i6e"},"source":["#Evaluation Functions\n","def accuracy(loader):\n","  correct = 0\n","  incorrect = 0\n","  count = 0\n","  for (input, label) in loader:\n","    if count%int(len(loader)*0.1) == 0:\n","      print(str(count) + \"/\" + str(len(loader)))\n","    count = count + 1\n","    input = input.to(device)\n","    pred = model(input)\n","    if(num_classes==1):\n","      if pred>=0.5:\n","        res = 1\n","      else:\n","        res = 0\n","      if res == label:\n","        correct = correct + 1\n","      else:\n","        incorrect = incorrect + 1\n","    elif(num_classes>1):\n","      if(label[0,torch.argmax(pred).item()].tolist()):\n","        correct = correct + 1\n","      else:\n","        incorrect = incorrect + 1\n","  print(float(correct/(correct+incorrect)))\n","  print(correct)\n","  print(incorrect)\n","  return (correct, incorrect)\n","\n","# following the evaluation metrics in https://en.wikipedia.org/wiki/Multi-label_classification\n","def F1ScoreCalculation(loader, model):\n","  threshold = 0.5\n","  T = 0       # size of set of truth labels\n","  P = 0       # size of set of predicated labels\n","  T_u_P = 0   # size of T union P\n","  T_n_P = 0   # size of T intersection P\n","  count = 0   # for iterating\n","  for (input, label) in loader:\n","    if count%int(len(loader)*0.1) == 0:\n","      print(str(count) + \"/\" + str(len(loader)))\n","    count = count + 1\n","    input = input.to(device)\n","    pred = model(input)\n","\n","    pred_list = pred.tolist()\n","    label_list = label.tolist()\n","    for i in range(num_classes):\n","      if num_classes > 1:\n","        if(pred_list[0][i] > threshold):\n","          P += 1\n","        if(label_list[0][i] > threshold):\n","          T += 1\n","        if(pred_list[0][i] > threshold and label_list[0][i] > threshold):\n","          T_n_P += 1\n","      elif num_classes == 1:\n","        if(pred_list[0][i] > threshold):\n","          P += 1\n","        if(label_list[0] > threshold):\n","          T += 1\n","        if(pred_list[0][i] > threshold and label_list[0] > threshold):\n","          T_n_P += 1\n","  T_u_P = T + P - T_n_P\n","\n","  print(\"total predicted label num:\", P)\n","  print(\"total truth label num:\", T)\n","\n","  if(T_n_P == 0):\n","    return 0\n","  assert P!=0, \"predicted label number is 0\"\n","  assert T!=0, \"truth label number is 0\"\n","\n","  precision = T_n_P/P\n","  recall = T_n_P/T\n","  F1 = 2/(1/precision + 1/recall)\n","  print(\"F1:\",F1)\n","  print(\"precision:\",precision)\n","  print(\"recall:\",recall)\n","  return (precision, recall, F1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bMmPcjXiLPj"},"source":["#if classification == \"type\" or classification == \"target\":\n","#  count = 0\n","#  for (input, label) in test_loader:\n","#    count += 1\n","#    input = input.to(device)\n","#    pred = model(input)\n","#    print(input)\n","#    print(pred)\n","#    print(label)\n","#    print(int(torch.argmax(pred).item()))\n","#    print(label[0,4].tolist())\n","#    if(count==10):\n","#      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIPDccOAO0cJ","outputId":"508ef7b6-7b0c-4577-92ce-a451ac279fe0"},"source":["print(len(vocab))\n","print(vocab(['<pad>'])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1034069\n","1\n"]}]},{"cell_type":"code","metadata":{"id":"r1ZgJFCz3mX-"},"source":["# define the RNN model \n","class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","    super(RNN, self).__init__()\n","    self.num_layers = num_layers\n","    self.hidden_size = hidden_size\n","    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)#, dropout = 0.2) #bidirectional\n","    if bidirectional:\n","      self.fc = nn.Linear(hidden_size*2, num_classes)\n","    else:\n","      self.fc = nn.Linear(hidden_size, num_classes)\n","    self.word_embedding = nn.Embedding(len(vocab), embed_dim, padding_idx=vocab(['<pad>'])[0])\n","    #self.word_embedding = nn.Embedding(1034266, embed_dim, padding_idx=1)  #calculated from default vocab this mag change over time (?)\n","    self.activation = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    out = self.word_embedding(x).to(device)\n","    if bidirectional:\n","      h0 = torch.autograd.Variable(torch.ones(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n","      c0 = torch.autograd.Variable(torch.ones(self.num_layers*2, x.size(0), self.hidden_size).to(device))\n","    else:\n","      h0 = torch.autograd.Variable(torch.ones(self.num_layers, x.size(0), self.hidden_size)).to(device)\n","      c0 = torch.autograd.Variable(torch.ones(self.num_layers, x.size(0), self.hidden_size).to(device))\n","    out,(h0,c0) = self.lstm(out, (h0,c0))\n","    out = out[:, -1, :]\n","    out = self.fc(out)\n","    out = self.activation(out)\n","    return out\n","\n","model = RNN(embed_dim, hidden_size, num_layer, num_classes).to(device)   # num_classes=2 because we are doing binary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYkwprKEK1Eh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fa74034-005d-487f-c4ba-da3cdb6b4bcc"},"source":["F1ScoreCalculation(test_loader, model) #F1 score before train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/480\n","48/480\n","96/480\n","144/480\n","192/480\n","240/480\n","288/480\n","336/480\n","384/480\n","432/480\n","total predicted label num: 486\n","total truth label num: 530\n","F1: 0.003937007874015748\n","precision: 0.00411522633744856\n","recall: 0.0037735849056603774\n"]},{"output_type":"execute_result","data":{"text/plain":["0.003937007874015748"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"jsCBJ_ip3ExZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ee85b4c-c0cb-4610-8e5e-cbb6956744ca"},"source":["# Train the RNN model\n","#criterion = nn.CrossEntropyLoss()\n","criterion = nn.BCELoss() #weight=class_weights\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n","\n","n_total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        # shape of \"inputs\" is [batch_size, max_len]\n","        # shape of \"labels\" is [batch_size, 1]\n","\n","        inputs = inputs.to(device)\n","        #labels = labels.reshape(-1).to(device)\n","        labels = labels.to(device, dtype=torch.float)\n","        labels = labels.reshape(labels.shape[0],num_classes)\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % int(len(train_loader)/4) == 0:\n","            count = 0\n","            loss_total = 0\n","            for (val_input, val_label) in val_loader: #validation loss calculation to see the progress of the training\n","              val_input = val_input.to(device)\n","              val_label = val_label.to(device, dtype=torch.float)\n","              val_label = val_label.reshape(val_label.shape[0],num_classes)\n","              val_output = model(val_input)\n","              loss_total = loss_total + criterion(val_output, val_label).item()\n","              count = count + 1\n","            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Train Loss: {loss.item():.10f}, Val Loss: {loss_total/count:.10f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/4], Step [684/2738], Train Loss: 0.2864159644, Val Loss: 0.2525187843\n","Epoch [1/4], Step [1368/2738], Train Loss: 0.2370384037, Val Loss: 0.2492833805\n","Epoch [1/4], Step [2052/2738], Train Loss: 0.1504172981, Val Loss: 0.1457506098\n","Epoch [1/4], Step [2736/2738], Train Loss: 0.1466296166, Val Loss: 0.1361152676\n","Epoch [2/4], Step [684/2738], Train Loss: 0.0826305971, Val Loss: 0.1327181523\n","Epoch [2/4], Step [1368/2738], Train Loss: 0.1053089648, Val Loss: 0.1264442786\n","Epoch [2/4], Step [2052/2738], Train Loss: 0.1232839301, Val Loss: 0.1213350939\n","Epoch [2/4], Step [2736/2738], Train Loss: 0.1109443605, Val Loss: 0.1212815364\n","Epoch [3/4], Step [684/2738], Train Loss: 0.0970210135, Val Loss: 0.1231976793\n","Epoch [3/4], Step [1368/2738], Train Loss: 0.1335400939, Val Loss: 0.1257446721\n","Epoch [3/4], Step [2052/2738], Train Loss: 0.1095059291, Val Loss: 0.1237182116\n","Epoch [3/4], Step [2736/2738], Train Loss: 0.1136402041, Val Loss: 0.1215718448\n","Epoch [4/4], Step [684/2738], Train Loss: 0.0948759541, Val Loss: 0.1346053615\n","Epoch [4/4], Step [1368/2738], Train Loss: 0.0894474760, Val Loss: 0.1352661221\n","Epoch [4/4], Step [2052/2738], Train Loss: 0.0875500888, Val Loss: 0.1396932673\n","Epoch [4/4], Step [2736/2738], Train Loss: 0.0844359919, Val Loss: 0.1342765839\n"]}]},{"cell_type":"code","metadata":{"id":"GKRB814QSwaM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79295a40-c9f5-406a-fe21-200856071d3c"},"source":["F1 = F1ScoreCalculation(test_loader, model) #accuracy after train\n","\n","notefile = '/content/drive/My Drive/Mochi/_multi_class_notes.txt'\n","log = \"\\n\"\n","if use_GLoVe:\n","  log += \"embed_dim: \" + str(embed_dim) + \"(in GLoVe)\\n\"\n","else:\n","  log += \"embed_dim: \" + str(embed_dim) + \"\\n\"\n","log += \"max_len: \" + str(max_len) + \"\\n\"\n","log += \"batch_size: \" + str(batch_size) + \"\\n\"\n","log += \"hidden_size: \" + str(hidden_size) + \"\\n\"\n","log += \"learning_rate: \" + str(learning_rate) + \"\\n\"\n","log += \"num_layer: \" + str(num_layer) + \"\\n\"\n","log += \"num_class: \" + str(num_classes) + \"\\n\"\n","log += \"num_epochs: \" + str(num_epochs) + \"\\n\"\n","log += \"precision: \" + str(F1[0]) + \"\\n\"\n","log += \"recall: \" + str(F1[1]) + \"\\n\"\n","log += \"F1: \" + str(F1[2]) + \"\\n\"\n","if use_GLoVe:\n","  log += \"using GLove embedding \\n\"\n","if bidirectional:\n","  log += \"using bidirectional LSTM \\n\"\n","myfile = open(notefile, 'a', encoding='utf-8')\n","myfile.write(log)\n","myfile.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/997\n","99/997\n","198/997\n","297/997\n","396/997\n","495/997\n","594/997\n","693/997\n","792/997\n","891/997\n","990/997\n","total predicted label num: 421\n","total truth label num: 509\n","F1: 0.8430107526881722\n","precision: 0.9311163895486936\n","recall: 0.7701375245579568\n"]}]}]}